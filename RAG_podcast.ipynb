{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Powered Document Processing and Retrieval\n",
    "\n",
    "This notebook demonstrates the process of document processing, information extraction, and answering questions using language and vector-based models. Libraries such as `LangChain`, `FAISS`, `Whisper`, and `HuggingFace`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting podcast from a Podcast Webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<audio controls=\"\" src=\"https://api.spreaker.com/v2/episodes/65095132/download.mp3\"></audio>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# URL strony z podcastem\n",
    "url = \"https://www.99twarzyai.pl/2025/03/25/ai-tinkerers-artur-wala/\"\n",
    "\n",
    "# Pobranie strony\n",
    "response = requests.get(url, verify = False)\n",
    "response.raise_for_status()\n",
    "\n",
    "# Analiza zawartości strony\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Znalezienie tagu audio\n",
    "audio_tag = soup.find(\"audio\")\n",
    "if audio_tag:\n",
    "    source_tag = audio_tag.find(\"source\")\n",
    "    if source_tag and source_tag.get(\"src\"):\n",
    "        print(\"Znaleziono źródło audio:\", source_tag[\"src\"])\n",
    "\n",
    "audio_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcribing an Audio File\n",
    "\n",
    "In this section, I use the `Whisper` model to transcribe an audio file into text. The `Whisper` model is an advanced speech processing model that supports multiple languages and works on CPUs, making it convenient for systems with limited hardware resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport whisper\\n\\nwhisper_model = whisper.load_model(\"medium\")\\nfile = \"podcast.mp3\"\\ntranscription = whisper_model.transcribe(file)\\ntranscription\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import whisper\n",
    "\n",
    "whisper_model = whisper.load_model(\"medium\")\n",
    "file = \"podcast.mp3\"\n",
    "transcription = whisper_model.transcribe(file)\n",
    "transcription\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Transcription to a File\n",
    "\n",
    "The transcription is saved to a text file so that it can be processed later. We use the `tempfile` module to manage temporary directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    transcription = whisper_model.transcribe(file)[\"text\"].strip()\n",
    "\n",
    "    with open(\"transcription.txt\", \"w\") as file:\n",
    "        file.write(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Transcription\n",
    "\n",
    "We load the saved text file so that it can be processed in the subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'W podkaście 99 twarzy AI rozmawiamy o wszystkim, co związane z technologią, innowacjami i oczywiście sztuczną inteligencją. Przygotuj się na luźne, inspirujące i często zaskakujące rozmowy z różnymi ekspertami, programistami, naukowcami, przedsiębiorcami i ludźmi, którzy na co dzień pracują z AI. Podcast 99 twarzy AI. Zaprasza Karl Stryja. Witajcie. Dzisiaj mam ogromną przyjemność rozmawiać z Artu'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"transcription.txt\" , \"r\") as file:\n",
    "    response = file.read()\n",
    "\n",
    "response[:400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Using an Embedding Model\n",
    "\n",
    "I use the `HuggingFaceEmbeddings` model to generate embeddings for the text. Embeddings are vector representations of text that enable efficient searching for similar documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "model = OllamaLLM(model = \"SpeakLeash/bielik-11b-v2.3-instruct:Q4_K_M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "            model_kwargs={'device': 'cpu'} ) # Use CPU for efficiency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Documents\n",
    "\n",
    "I use `TextLoader` to load text documents that will be processed in the following steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "file_path = \"transcription.txt\"\n",
    "\n",
    "loader = TextLoader(file_path)\n",
    "text_document = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Text into Chunks\n",
    "\n",
    "I use `RecursiveCharacterTextSplitter` to split the text into smaller chunks. This allows us to efficiently process large documents and search for information within them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50 , length_function=len, add_start_index=True)\n",
    "documents = text_splitter.split_documents(text_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Vector Store\n",
    "\n",
    "Using `FAISS`, I create a vector store that allows for fast searching of similar documents based on user queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "że to jest taki aspekt kulturowy. Na następnym AI Tinkers seria porażek? Ciekawy pomysł do rozważenia. Zobaczymy, kiedy będzie kolejny meetup. Nie chcę jeszcze obiecywać kiedy, bo teraz musimy się skupić na hackathonie z OpenAI. To jest jeszcze taki top priority dla nas. Właśnie bardzo płynnie przyszedłeś do tematu, który chciałem poruszyć. Pamiętam naszą rozmowę chyba 2 albo 3 tygodnie temu. Jak ze sobą korespondowaliśmy, pisałeś Karol będzie grubo. No i co, dowiozłem. Gratuluję, Artur.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "query = \"Co to jest AI tinkers?\"\n",
    "\n",
    "db = FAISS.from_documents(documents , embedding=embeddings_model)\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Prompt template and Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Odpowiedz na pytanie wyłącznie w oparciu o poniższy kontekst. Bądź zwięzły.\n",
    "    Jeśli nie możesz znaleźć odpowiedzi w kontekście, powiedz „Nie mogę odpowiedzieć na to pytanie na podstawie podanego kontekstu”.\n",
    "        \n",
    "    Kontekst: {context}\n",
    "    Pytanie: {question}\n",
    "    Odpowiedź: \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Query Processing Chain\n",
    "\n",
    "I create a query processing chain that combines searching in the vector store with a language model. This allows us to generate answers to user questions based on the context of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AI Tinkers to seria spotkań lub wydarzeń związanych z sztuczną inteligencją, na których uczestnicy mogą dzielić się swoimi projektami, pomysłami i doświadczeniami w tej dziedzinie. Nie jest to jednak oficjalna nazwa ani organizacja; wydaje się, że jest to nieformalna inicjatywa lub cykl spotkań zainteresowanych osób.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "retriver =  db.as_retriever(search_type=\"similarity\")\n",
    "\n",
    "# def format_docs(docs):\n",
    "#    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "chain = (\n",
    "    {\"context\":retriver | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "chain.invoke(\"Co to jest AI tinkers??\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
